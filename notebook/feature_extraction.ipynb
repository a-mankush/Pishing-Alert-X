{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_B_05_2020.csv\")\n",
    "url_df = df[['url', \"status\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.crestonwood.com/router.php</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://shadetreetechnology.com/V4/validation/a...</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://support-appleld.com.secureupdate.duila...</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://rgipt.ac.in</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.iracing.com/tracks/gateway-motorspo...</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url      status\n",
       "0              http://www.crestonwood.com/router.php  legitimate\n",
       "1  http://shadetreetechnology.com/V4/validation/a...    phishing\n",
       "2  https://support-appleld.com.secureupdate.duila...    phishing\n",
       "3                                 http://rgipt.ac.in  legitimate\n",
       "4  http://www.iracing.com/tracks/gateway-motorspo...  legitimate"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "except Exception as e:\n",
    "    response = \"\"\n",
    "    soup = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_url\n",
    "url_df['url'].apply(lambda x: len(x))\n",
    "\n",
    "# length_hostname\n",
    "url_df['url'].apply(lambda x: len(x.split(\"/\")[2]))\n",
    "\n",
    "# nb_dot\n",
    "url_df['url'].apply(lambda x: x.count(\".\"))\n",
    "\n",
    "# nb_hyphens\n",
    "url_df['url'].apply(lambda x: x.count(\"-\"))\n",
    "\n",
    "# nb_at\n",
    "url_df['url'].apply(lambda x: x.count(\"@\"))\n",
    "\n",
    "# nb_qm\n",
    "url_df['url'].apply(lambda x: x.count(\"?\"))\n",
    "\n",
    "# nb_and\n",
    "url_df['url'].apply(lambda x: x.count(\"&\"))\n",
    "\n",
    "# nb_eq\n",
    "url_df['url'].apply(lambda x: x.count(\"=\"))\n",
    "\n",
    "# nb_underscore\n",
    "url_df['url'].apply(lambda x: x.count(\"-\"))\n",
    "\n",
    "# nb_tilde\n",
    "url_df['url'].apply(lambda x: x.count(\"~\"))\n",
    "\n",
    "# nb_percentage\n",
    "url_df['url'].apply(lambda x: x.count(\"%\"))\n",
    "\n",
    "# nb_slash\n",
    "url_df['url'].apply(lambda x: x.count(\"/\"))\n",
    "\n",
    "# nb_star\n",
    "# Note: We only have 8 url with stars in our data set\n",
    "url_df['url'].apply(lambda x: x.count(\"*\"))\n",
    "\n",
    "# nb_colon\n",
    "url_df['url'].apply(lambda x: x.count(\":\"))\n",
    "\n",
    "# nb_comma\n",
    "url_df['url'].apply(lambda x: x.count(\",\"))\n",
    "\n",
    "# nb_semicolumn\n",
    "url_df['url'].apply(lambda x: x.count(\";\"))\n",
    "\n",
    "# nb_doller\n",
    "url_df['url'].apply(lambda x: x.count(\"$\"))\n",
    "\n",
    "# nb_space\n",
    "# Note: Result is not matching\n",
    "url_df['url'].apply(lambda x: x.count(\" \"))\n",
    "\n",
    "# nb_www\n",
    "url_df['url'].apply(lambda x: x.count(\"www\"))\n",
    "\n",
    "# nb_com\n",
    "url_df['url'].apply(lambda x: x.count(\"com\"))\n",
    "\n",
    "# nb_dslash\n",
    "# Note: Result is not matching\n",
    "url_df['url'].apply(lambda x: x.count(\"//\"))\n",
    "\n",
    "# https token\n",
    "url_df['url'].apply(lambda x: 0 if \"https\" in x else 1)\n",
    "\n",
    "# http_in_path \n",
    "# Note some percentage of values are not matching (but maximum are)\n",
    "url_df['url'].apply(lambda url: (\"\".join(url.split('/')[1:])).count(\"http\"))\n",
    "\n",
    "\n",
    "# shortening_service\n",
    "def shortening_service(full_url):\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net',\n",
    "                      full_url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def get_domain_age(url):\n",
    "    try:\n",
    "        # Get the whois information for the domain\n",
    "        domain_info = whois.whois(url)\n",
    "\n",
    "        # Extract the creation date from the whois information\n",
    "        creation_date = domain_info.creation_date\n",
    "\n",
    "        # Check if the creation date is a list and get the first element\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "\n",
    "        # Calculate the age of the domain in years\n",
    "        if creation_date:\n",
    "            today = datetime.now()\n",
    "            age = today.year - creation_date.year - ((today.month, today.day) < (creation_date.month, creation_date.day))\n",
    "            return age\n",
    "        else:\n",
    "            return \"Creation date not available\"\n",
    "\n",
    "    except whois.parser.PywhoisError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "url = \"http://www.crestonwood.com/router.php\t\"\n",
    "domain_age = get_domain_age(url)\n",
    "\n",
    "print(f\"The age of the domain {url} is: {domain_age}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def is_indexed_by_google(url):\n",
    "    try:\n",
    "        # Send a GET request to Google with the site: operator\n",
    "        response = requests.get(f'https://www.google.com/search?q=site:{url}')\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Check if the search results contain the website URL\n",
    "            search_results = soup.find_all('a', href=True)\n",
    "            for result in search_results:\n",
    "                if url.lower() in result['href'].lower():\n",
    "                    return 1\n",
    "            return 0\n",
    "        else:\n",
    "            return 0  # Return 0 for failure to retrieve data\n",
    "\n",
    "    except Exception as e:\n",
    "        return 0  # Return 0 for errors\n",
    "\n",
    "# Example usage\n",
    "website_url = \"example.com\"\n",
    "result = is_indexed_by_google(website_url)\n",
    "\n",
    "print(f\"The website {website_url} is{' ' if result else ' not '}indexed by Google.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dns.resolver\n",
    "\n",
    "def get_dns_records(domain):\n",
    "    try:\n",
    "        # Use the default resolver\n",
    "        resolver = dns.resolver.Resolver()\n",
    "\n",
    "        # Get all DNS records for the domain\n",
    "        response = resolver.resolve(domain, 'A')  # 'A' record for IPv4 addresses\n",
    "        a_records = [record.address for record in response]\n",
    "\n",
    "        response = resolver.resolve(domain, 'AAAA')  # 'AAAA' record for IPv6 addresses\n",
    "        aaaa_records = [record.address for record in response]\n",
    "\n",
    "        response = resolver.resolve(domain, 'MX')  # 'MX' record for mail servers\n",
    "        mx_records = [(record.preference, str(record.exchange)) for record in response]\n",
    "\n",
    "        response = resolver.resolve(domain, 'NS')  # 'NS' record for name servers\n",
    "        ns_records = [str(record.target) for record in response]\n",
    "\n",
    "        response = resolver.resolve(domain, 'CNAME')  # 'CNAME' record for aliases\n",
    "        cname_records = [str(record.target) for record in response]\n",
    "\n",
    "        return {\n",
    "            'A': a_records,\n",
    "            'AAAA': aaaa_records,\n",
    "            'MX': mx_records,\n",
    "            'NS': ns_records,\n",
    "            'CNAME': cname_records\n",
    "        }\n",
    "\n",
    "    except dns.resolver.NXDOMAIN:\n",
    "        return f\"Domain '{domain}' does not exist.\"\n",
    "    except dns.exception.Timeout:\n",
    "        return \"Timeout error during DNS query.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "domain_to_check = \"http://www.ktplasmachinery.com/cs/\"\n",
    "dns_records = get_dns_records(domain_to_check)\n",
    "\n",
    "print(f\"DNS records for {domain_to_check}:\\n{dns_records}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def has_empty_title(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the title tag\n",
    "            title_tag = soup.find('title')\n",
    "\n",
    "            # Return 1 if the title is empty, otherwise return 0\n",
    "            return 1 if title_tag is not None and not title_tag.text.strip() else 0\n",
    "        \n",
    "        else:\n",
    "            return 0  # Return 0 for failure to retrieve data\n",
    "\n",
    "    except Exception as e:\n",
    "        return 0  # Return 0 for errors\n",
    "\n",
    "# Example usage\n",
    "website_url = \"http://html.house/l7ceeid6.html\"\n",
    "result = has_empty_title(website_url)\n",
    "\n",
    "print(f\"The website {website_url} has an {'empty' if result else 'non-empty'} title.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_url_features(url):\n",
    "    features = {}\n",
    "    \n",
    "    # length_url\n",
    "    features['length_url'] = len(url)\n",
    "    \n",
    "    # length_hostname\n",
    "    features['length_hostname'] = len(url.split(\"/\")[2])\n",
    "    \n",
    "    # nb_dot\n",
    "    features['nb_dot'] = url.count(\".\")\n",
    "    \n",
    "    # nb_hyphens\n",
    "    features['nb_hyphens'] = url.count(\"-\")\n",
    "    \n",
    "    # nb_at\n",
    "    features['nb_at'] = url.count(\"@\")\n",
    "    \n",
    "    # nb_qm\n",
    "    features['nb_qm'] = url.count(\"?\")\n",
    "    \n",
    "    # nb_and\n",
    "    features['nb_and'] = url.count(\"&\")\n",
    "    \n",
    "    # nb_eq\n",
    "    features['nb_eq'] = url.count(\"=\")\n",
    "    \n",
    "    # nb_underscore\n",
    "    features['nb_underscore'] = url.count(\"_\")\n",
    "    \n",
    "    # nb_tilde\n",
    "    features['nb_tilde'] = url.count(\"~\")\n",
    "    \n",
    "    # nb_percentage\n",
    "    features['nb_percentage'] = url.count(\"%\")\n",
    "    \n",
    "    # nb_slash\n",
    "    features['nb_slash'] = url.count(\"/\")\n",
    "    \n",
    "    # nb_star\n",
    "    features['nb_star'] = url.count(\"*\")\n",
    "    \n",
    "    # nb_colon\n",
    "    features['nb_colon'] = url.count(\":\")\n",
    "    \n",
    "    # nb_comma\n",
    "    features['nb_comma'] = url.count(\",\")\n",
    "    \n",
    "    # nb_semicolumn\n",
    "    features['nb_semicolumn'] = url.count(\";\")\n",
    "    \n",
    "    # nb_doller\n",
    "    features['nb_doller'] = url.count(\"$\")\n",
    "    \n",
    "    # nb_space\n",
    "    features['nb_space'] = url.count(\" \")\n",
    "    \n",
    "    # nb_www\n",
    "    features['nb_www'] = url.count(\"www\")\n",
    "    \n",
    "    # nb_com\n",
    "    features['nb_com'] = url.count(\"com\")\n",
    "    \n",
    "    # nb_dslash\n",
    "    features['nb_dslash'] = url.count(\"//\")\n",
    "    \n",
    "    # https token\n",
    "    features['https_token'] = 0 if \"https\" in url else 1\n",
    "    \n",
    "    # http_in_path\n",
    "    features['http_in_path'] = (\"\".join(url.split('/')[1:])).count(\"http\")\n",
    "    \n",
    "    # shortening_service\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net',\n",
    "                      url)\n",
    "    features['shortening_service'] = 1 if match else 0\n",
    "    \n",
    "    return pd.DataFrame(features, index=[0])\n",
    "\n",
    "# Example usage\n",
    "url_to_check = \"http://www.example.com\"\n",
    "url_features = create_url_features(url_to_check)\n",
    "\n",
    "print(f\"URL features for {url_to_check}:\\n{url_features.columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_alexa_rank(url):\n",
    "    try:\n",
    "        # Try to fetch the Alexa rank data for the given URL\n",
    "        rank = BeautifulSoup(urllib.request.urlopen(\n",
    "            \"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        \n",
    "        # Convert the rank to an integer\n",
    "        rank = int(rank)\n",
    "\n",
    "        # Check if the rank is below 100,000\n",
    "        if rank < 100000:\n",
    "            # If the rank is below 100,000, return 1\n",
    "            return 1\n",
    "        else:\n",
    "            # If the rank is 100,000 or above, return 0\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        # If there is an exception (e.g., unable to fetch data), return -1\n",
    "        return -1\n",
    "\n",
    "# Example usage:\n",
    "url_to_check = \"example.com\"\n",
    "result = get_alexa_rank(url_to_check)\n",
    "\n",
    "if result == 1:\n",
    "    print(f\"The website {url_to_check} has an Alexa rank below 100,000.\")\n",
    "elif result == 0:\n",
    "    print(f\"The website {url_to_check} has an Alexa rank 100,000 or above.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve Alexa rank for {url_to_check}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
